{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxizheng/miniconda3/envs/get_responses/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import warnings\n",
    "import logging\n",
    "from datasets import Dataset\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pattern.en import pluralize\n",
    "import pickle5 as pickle\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "import openai\n",
    "import transformers\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time):\n",
    "    \"\"\"the helper function for running time\"\"\"\n",
    "    minutes, seconds = divmod(time.time() - start_time, 60)\n",
    "    return f\"{int(minutes)} mins {int(seconds)} sec\"\n",
    "\n",
    "def send_gpt_prompt(batch, model_type, prompt_and_response, temperature, max_tokens):\n",
    "    \"\"\"helper function to send a whole to chatgpt\"\"\"\n",
    "    for prompt in batch:\n",
    "        succeed = False\n",
    "        completion = None\n",
    "        while not succeed:\n",
    "            try:\n",
    "                completion = openai.Completion.create(\n",
    "                    engine = model_type,\n",
    "                    messages = prompt,\n",
    "                    max_tokens = max_tokens,\n",
    "                    n = 1,\n",
    "                    temperature = temperature,\n",
    "                )\n",
    "                succeed = True\n",
    "            except Exception as e:\n",
    "                print(\"GPT sleeping...\", e)\n",
    "                sleep(60)\n",
    "        assert completion is not None\n",
    "        response = completion['choices'][0]['text'].replace('\\n', ' ').replace(' .', '.').strip()\n",
    "        prompt_and_response.append([prompt, response])\n",
    "        \n",
    "def generate_responses_gpt(batches, model_type, output_path, temperature, max_tokens):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    openai.api_key = Path(f\"../AZURE_OPENAI_KEY\").read_text()\n",
    "    openai.api_base = Path(f\"../AZURE_OPENAI_ENDPOINT\").read_text()\n",
    "    openai.api_type = 'azure'\n",
    "    openai.api_version = '2023-03-15-preview'\n",
    "    # openai.api_version = '2023-05-15'\n",
    "        \n",
    "    prompt_and_response = []\n",
    "    \n",
    "    # can change n_jobs accoding to the size of dataset\n",
    "    Parallel(n_jobs = 10, require='sharedmem')(\n",
    "        delayed(send_gpt_prompt)(\n",
    "            batch, model_type, prompt_and_response, temperature, max_tokens\n",
    "        ) for batch in batches\n",
    "    )\n",
    "    \n",
    "    print(f'Time taken to generate responses is {timer(start_time)}s')\n",
    "    \n",
    "    # write_responses(prompt_and_response, output_path, 'w')\n",
    "    \n",
    "    return prompt_and_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT sleeping... Invalid response object from API: 'Unsupported data type\\n' (HTTP response code was 400)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m batches \u001b[39m=\u001b[39m [[\n\u001b[1;32m      2\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mAnswer with only one number from 1 to 7, considering 1 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mextremely dissimilar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 2 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvery dissimilar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 3 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlikely dissimilar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 4 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mneutral\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 5 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlikely similar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 6 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvery similar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, and 7 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mextremely similar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: How similar is Alligator and Alligator?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mAnswer with only one number from 1 to 7, considering 1 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mextremely dissimilar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 2 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvery dissimilar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 3 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlikely dissimilar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 4 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mneutral\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 5 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlikely similar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 6 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvery similar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, and 7 as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mextremely similar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: How similar is Alligator and Blindworm?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39m# \"Answer using only one word - Gecko or Salamander and not Alligator. Which is more similar in meaning to Alligator? End your answer with, 'The answer is '. Let's think step by step.\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ]]\n\u001b[0;32m----> 9\u001b[0m generate_responses_gpt(batches, \u001b[39m\"\u001b[39;49m\u001b[39mgpt-35-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m./test.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m0.7\u001b[39;49m, \u001b[39m256\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m, in \u001b[0;36mgenerate_responses_gpt\u001b[0;34m(batches, model_type, output_path, temperature, max_tokens)\u001b[0m\n\u001b[1;32m     37\u001b[0m prompt_and_response \u001b[39m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m \u001b[39m# can change n_jobs accoding to the size of dataset\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m Parallel(n_jobs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, require\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m'\u001b[39;49m)(\n\u001b[1;32m     41\u001b[0m     delayed(send_gpt_prompt)(\n\u001b[1;32m     42\u001b[0m         batch, model_type, prompt_and_response, temperature, max_tokens\n\u001b[1;32m     43\u001b[0m     ) \u001b[39mfor\u001b[39;49;00m batch \u001b[39min\u001b[39;49;00m batches\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTime taken to generate responses is \u001b[39m\u001b[39m{\u001b[39;00mtimer(start_time)\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39m# write_responses(prompt_and_response, output_path, 'w')\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/get_responses/lib/python3.8/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/miniconda3/envs/get_responses/lib/python3.8/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/miniconda3/envs/get_responses/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/get_responses/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/get_responses/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/get_responses/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT sleeping... Invalid response object from API: 'Unsupported data type\\n' (HTTP response code was 400)\n",
      "GPT sleeping... Invalid response object from API: 'Unsupported data type\\n' (HTTP response code was 400)\n",
      "GPT sleeping... Invalid response object from API: 'Unsupported data type\\n' (HTTP response code was 400)\n",
      "GPT sleeping... Invalid response object from API: 'Unsupported data type\\n' (HTTP response code was 400)\n",
      "GPT sleeping... Invalid response object from API: 'Unsupported data type\\n' (HTTP response code was 400)\n"
     ]
    }
   ],
   "source": [
    "batches = [[\n",
    "\"Answer with only one number from 1 to 7, considering 1 as 'extremely dissimilar', 2 as 'very dissimilar', 3 as 'likely dissimilar', 4 as 'neutral', 5 as 'likely similar', 6 as 'very similar', and 7 as 'extremely similar': How similar is Alligator and Alligator?\",\n",
    "\"Answer with only one number from 1 to 7, considering 1 as 'extremely dissimilar', 2 as 'very dissimilar', 3 as 'likely dissimilar', 4 as 'neutral', 5 as 'likely similar', 6 as 'very similar', and 7 as 'extremely similar': How similar is Alligator and Blindworm?\",\n",
    "\"Answer with only one number from 1 to 7, considering 1 as 'extremely dissimilar', 2 as 'very dissimilar', 3 as 'likely dissimilar', 4 as 'neutral', 5 as 'likely similar', 6 as 'very similar', and 7 as 'extremely similar': How similar is Alligator and Boapython?\",\n",
    "    # \"Answer using only one word - Alligator or Spanner and not Shovel. Which is more similar in meaning to Shovel? End your answer with, 'The answer is '. Let's think step by step.\",\n",
    "    # \"Answer using only one word - Gecko or Salamander and not Alligator. Which is more similar in meaning to Alligator? End your answer with, 'The answer is '. Let's think step by step.\"\n",
    "]]\n",
    "\n",
    "generate_responses_gpt(batches, \"gpt-35-turbo\", './test.csv', 0.7, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to generate responses is 0 mins 4 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\"Answer using only one word - Alligator or Spanner and not Shovel. Which is more similar in meaning to Shovel? End your answer with, 'The answer is '. Let's think step by step.\",\n",
       "  'A shovel is a tool that is typically used for moving dirt or for digging. A spanner is also a type of tool, often used for mechanical construction. The answer is Spanner.',\n",
       "  'Spanner'],\n",
       " [\"Answer using only one word - Gecko or Salamander and not Alligator. Which is more similar in meaning to Alligator? End your answer with, 'The answer is '. Let's think step by step.\",\n",
       "  \"The word 'alligator' is usually associated with a reptile, so the other two words must also be reptiles. 'Salamander' is a type of amphibian and 'gecko' is a type of lizard. Therefore, the answer is Gecko. The answer is Gecko.\",\n",
       "  'Gecko'],\n",
       " [\"Answer using only one word - Alligator or Spanner and not Shovel. Which is more similar in meaning to Shovel? End your answer with, 'The answer is '. Let's think step by step.\",\n",
       "  'The answer is Spanner.',\n",
       "  'Spanner'],\n",
       " [\"Answer using only one word - Gecko or Salamander and not Alligator. Which is more similar in meaning to Alligator? End your answer with, 'The answer is '. Let's think step by step.\",\n",
       "  'Alligator is a reptile, so any reptile would be a similar meaning. Thus, the answer is Gecko.',\n",
       "  'Gecko']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_responses_gpt(batches, \"text-davinci-003\", './test.csv', 0.7, 256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "get_responses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
